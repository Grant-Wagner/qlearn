# qlearn
This is a little practice program for learning about Q learning. Initially drafted up based on a conversation w/ SD about the best DS approach for teaching some agent to intelligently bet based on time series data of bookmaking odds. Thus the first environment for a q agent was a simple sequence, generated by the generateSequence method, which was then passed in to a Simulation object which could communicate with an Agent object.

Of course, this is a pretty silly idea after you think about it for more than a second, which suggests I should think about things for more than a second. Since the agent isn't actually making an impact on the random sequence it learns a bunch of ridiculous things. This is because the q learning paradigm associates the next number in the sequence being 4 (for example) with the action the agent took on the last value of the sequence, attributing causality where there is none. Lessons about making bad bets in the hope that they will "cause" future good bets propagate through the q table of the agent and hilarity ensues. The fix for this problem is to set the gamma (future reward discount) parameter of the agent to 0, nullifying the whole point of the RL.

I was still interested in the whole Q learning thing though, so I moved on from the intended use case to a grid based simulation where an agent steps through a simple grid looking for candy. That part of the simulation is encapsulated by the Grid and SimulationPrime classes. Fun project - it's massive overkill and horribly coupled and unoptimized but I did learn a fair bit about the whole Q learning idea. Playing around with making rewards proportional to the inverse of the manhattan distance of the agent from the reward tile was neat and I still want to better understand some of the mathematics.

Future Possibilities:

<ul>
(i) SD developed a similar agent that avoids hazards on a grid. This got me thinking about whether two agents that have learned different lessons about similar environments could be combined - i.e. if one agent reacts to hazards based on a "sensor" that it has and another seeks rewards, can we reliably merge those two behaviors into one "super agent"? The answer is certainly yes and will be fun to code up if/when I have the motivation. Sticking points are appropriately weighting the two behaviors (read q-tables) and having a good understanding of the distribution of possible q-values in each table.


(ii) Introduce uncertainty into the simulation. With uncertain rewards, punishments or states we need to move up from a simple table to a DQN (maybe). Haven't thought this one through entirely but the key idea is to equip the agent with a sensor that outputs probabilities of hazards or rewards in each direction rather than certain hazards and rewards.

(iii) Combining the ideas from (i) and (ii), how could we reliably combine DQNs?

(iv) For some reason my algebra keeps on coming out wrong and I really feel that the expected reward of going right when directly to the left of the goal tile should be 1 + gamma. Simulations show that it is just gamma. Fix the bug in the code or brain that is causing this and do some more investigation of why the expected rewards look like polynomials in gamma.

(v) Fix venv.
</ul>

Things Learned:

<ul>
(i) There are better libraries and APIs that already implement GridWorld that I could have used. It was fun writing my own classes but might be worth doing more preliminary research and documentation reading in the future since I am clearly bad at the architecture portion of this stuff.


(ii) There is a super cool betting API from betfair that could generate huge training sets for some future project on time series data.

(iii) Mentioned above, the q-values follow a somewhat predictable expressions as polynomials in gamma. Probably some actual mathematics could reveal a neat pattern and closed form solutions for problems in GridWorld.

(iv) Good matplotlib code snippets in here for future Grant.
</ul>